{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30bc93f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import re\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a63b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc163f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.utils import clean_wiki_text, convert_files_to_dicts, fetch_archive_from_http, print_answers\n",
    "from haystack.nodes import FARMReader, TransformersReader\n",
    "from haystack.nodes import TextConverter, PDFToTextConverter, DocxToTextConverter, PreProcessor\n",
    "from haystack.pipelines import ExtractiveQAPipeline\n",
    "\n",
    "#haystack contains a search system for retrieval and QA across documents.\n",
    "#designed for large documents, but pipeline also works for single document QA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6ab6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In-Memory Document Store\n",
    "from haystack.document_stores import InMemoryDocumentStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4dc9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve from store\n",
    "from haystack.nodes import TfidfRetriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8928dda",
   "metadata": {},
   "source": [
    "# import file containing all abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66bb980",
   "metadata": {},
   "outputs": [],
   "source": [
    "fulldf = pd.read_csv('data/methods_extracted.csv', index_col=0)\n",
    "fulldf['methods'] = fulldf['methods'].str.replace(\"\\n\",' ')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7fb9b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1393f13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fulldf_m = fulldf[fulldf['methods'].notnull()].reset_index(drop=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18deeeda",
   "metadata": {},
   "outputs": [],
   "source": [
    "methods_list = fulldf_m['methods'].tolist()\n",
    "title_list = list(range(0,len(methods_list)))\n",
    "\n",
    "pmid_list = fulldf_m['pmid'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b2e498",
   "metadata": {},
   "outputs": [],
   "source": [
    "fulldf_m.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792e4172",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "methods_list[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b732ef73",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(len(methods_list))\n",
    "print(len(fulldf_m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578ba18d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#test match\n",
    "#n = 9\n",
    "#\n",
    "#test = methods_list[n]\n",
    "#pmid = pmid_list[n]\n",
    "#test_dict = {'content': test, 'meta': {'name': pmid}}\n",
    "#\n",
    "#test_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91128e36",
   "metadata": {},
   "source": [
    "# set up question bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e922a3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#q1=\"what disease is being studied?\"\n",
    "#q2=\"What is the objective of the study?\"\n",
    "###\n",
    "q3=\"how many patient data samples were included in this study?\"\n",
    "#q4=\"what modality of data is used in this study?\"\n",
    "###\n",
    "#q5=\"what country was the study conducted in?\"\n",
    "#q6=\"what hospital did the data come from?\"\n",
    "q7=\"What existing data source did the data come from?\"\n",
    "q8=\"What location did the data come from?\"\n",
    "###\n",
    "#q10=\"how does the model perform relative to a human?\"\n",
    "#q11=\"how does the model perform in prospective testing\"\n",
    "#q12=\"what were the results of the study?\"\n",
    "#q12=\"what was the area under the curve (AUC) value?\"\n",
    "qlist = [q3, q7, q8]\n",
    "#qlist = [q1, q2, q3, q4, q5, q6, q7, q8]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bade33d",
   "metadata": {},
   "source": [
    "# initialising pre-processor module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5108f91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = PreProcessor(\n",
    "    clean_empty_lines=True,\n",
    "    clean_whitespace=True,\n",
    "    clean_header_footer=False,\n",
    "    split_by=\"word\",\n",
    "    split_length=2000,\n",
    "    split_respect_sentence_boundary=True,\n",
    ")\n",
    "\n",
    "document_store = InMemoryDocumentStore()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880131d6",
   "metadata": {},
   "source": [
    "# hugging face model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b1e670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reader scans text returned by retriever and extracts k-best answers\n",
    "# Load a fine-tuned  model (e.g. RoBERTa QA = \"deepset/roberta-base-squad2\")\n",
    "# alternatives (Reader): TransformersReader (leveraging the pipeline of the Transformers package)\n",
    "# alternatives (Models): e.g. \"distilbert-base-uncased-distilled-squad\" (fast) or \"deepset/bert-large-uncased-whole-word-masking-squad2\" (good accuracy)\n",
    "# can adjust the model to return \"no answer possible\" with the no_ans_boost. Higher values mean the model prefers \"no answer possible\"\n",
    "# alternatively, QA models on model hub (https://huggingface.co/models)\n",
    "#sota: ahotrod/albert_xxlargev1_squad2_512\n",
    "#dmis-lab/biobert-large-cased-v1.1-squad\n",
    "#\n",
    "#reader = FARMReader(model_name_or_path=\"deepset/roberta-base-squad2\", use_gpu=True, use_confidence_scores=True)\n",
    "#sets pipeline to contain retriever and reader\n",
    "#pipe = ExtractiveQAPipeline(reader, retriever)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b93070",
   "metadata": {},
   "source": [
    "# question answer pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90569010",
   "metadata": {},
   "outputs": [],
   "source": [
    "##create dataframe to hold results\n",
    "\n",
    "resultsdf = fulldf_m[['pmid', 'title', 'methods']].copy()\n",
    "\n",
    "for q in qlist:\n",
    "    resultsdf[q] = pd.Series(dtype='object')\n",
    "    \n",
    "#resultsdf['q1_disease'] = pd.Series(dtype='object')\n",
    "#resultsdf['q2_objective'] = pd.Series(dtype='object')\n",
    "#resultsdf['q3_size'] = pd.Series(dtype='object')\n",
    "#resultsdf['q4_modality'] = pd.Series(dtype='object')\n",
    "#resultsdf['q5_country'] = pd.Series(dtype='object')\n",
    "#resultsdf['q6_hospital'] = pd.Series(dtype='object')\n",
    "#resultsdf['q7_database'] = pd.Series(dtype='object')\n",
    "#resultsdf['q8_organisation'] = pd.Series(dtype='object')\n",
    "\n",
    "print(len(resultsdf))\n",
    "resultsdf.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08745d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#qa_df = resultsdf[0:50]\n",
    "qa_df = resultsdf.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0f2b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = TfidfRetriever(document_store=document_store)\n",
    "tuned_reader = FARMReader(model_name_or_path=\"pubmed_tuned_methods\", use_gpu=True, use_confidence_scores=True)\n",
    "tuned_pipe = ExtractiveQAPipeline(tuned_reader, retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6893c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "for i, row in tqdm(qa_df.iterrows()):\n",
    "    try:\n",
    "        #set up dict for new methods\n",
    "        methods_dict = {'content': methods_list[i], 'meta': {'name': title_list[i]}}\n",
    "        docs_proc = preprocessor.process(methods_dict)\n",
    "            #####test#####print(f\"n_docs_input: 1\\nn_docs_output: {len(docs_proc)}\")\n",
    "    \n",
    "        #dump old doc store, and import next document into doc store\n",
    "        document_store.delete_documents()\n",
    "        document_store.write_documents(docs_proc)\n",
    "    \n",
    "        #set up pipeline\n",
    "        retriever = TfidfRetriever(document_store=document_store)\n",
    "        tuned_pipe = ExtractiveQAPipeline(tuned_reader, retriever)\n",
    "        \n",
    "        #run pipeline for current 'i'\n",
    "        plist = qlist.copy() #same length list to set-up iteration through question bank\n",
    "        l = len(qlist)\n",
    "    \n",
    "        temp_list = []\n",
    "        temp_list = [row['pmid'], row['title'], row['methods']]\n",
    "        \n",
    "        try:\n",
    "            for answer in range(0,l):\n",
    "                plist[answer] = tuned_pipe.run(\n",
    "                    query=qlist[answer], params={\"Retriever\": {\"top_k\": 10}, \"Reader\": {\"top_k\": 5}}\n",
    "                )\n",
    "        \n",
    "            #append top answer for each row/question\n",
    "            #print(plist[answer]['answers'][0])\n",
    "                temp_list.append(plist[answer]['answers'][0])\n",
    "        \n",
    "            qa_df.loc[i] = temp_list\n",
    "    \n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "#measure time\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6b7205",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_df.to_csv('output/methods_interim.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b2fc60",
   "metadata": {},
   "source": [
    "# clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ebb0ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cleandf = qa_df.copy().applymap(str)\n",
    "cleandf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ef6119",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create lists for answer/score pairs\n",
    "disease_score = []\n",
    "disease_answer = []\n",
    "question_score = []\n",
    "question_answer = []\n",
    "sample_score = []\n",
    "sample_answer = []\n",
    "modality_score = []\n",
    "modality_answer = []\n",
    "country_score = []\n",
    "country_answer = []\n",
    "hospital_score = []\n",
    "hospital_answer = []\n",
    "database_score = []\n",
    "database_answer = []\n",
    "organisation_score = []\n",
    "organisation_answer = []\n",
    "\n",
    "#categories = [disease_score, disease_answer, sample_score, sample_answer, modality_score, modality_answer,\n",
    "#              country_score, country_answer, hospital_score, hospital_answer, database_score, database_answer,\n",
    "#              organisation_score, organisation_answer]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4556a5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = cleandf.iloc[34,5]\n",
    "\n",
    "## start/end strings for scores and answers\n",
    "score_start = 'score='\n",
    "score_end = ', context'\n",
    "print((s.split(score_start))[1].split(score_end)[0])\n",
    "\n",
    "answer_start = 'answer='\n",
    "answer_end = ', score'\n",
    "print((s.split(answer_start))[1].split(answer_end)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9071ccdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in tqdm(cleandf.iterrows()):\n",
    "    try:\n",
    "        #disease_score.append((row['what disease is being studied?'].split(score_start))[1].split(score_end)[0])\n",
    "        #disease_answer.append((row['what disease is being studied?'].split(answer_start))[1].split(answer_end)[0])\n",
    "        sample_score.append((row['how many patient data samples were included in this study?'].split(score_start))[1].split(score_end)[0])\n",
    "        sample_answer.append((row['how many patient data samples were included in this study?'].split(answer_start))[1].split(answer_end)[0])\n",
    "        #question_score.append((row['What is the objective of the study?'].split(score_start))[1].split(score_end)[0])\n",
    "        #question_answer.append((row['What is the objective of the study?'].split(answer_start))[1].split(answer_end)[0])\n",
    "        #modality_score.append((row['what modality of data is used in this study?'].split(score_start))[1].split(score_end)[0])\n",
    "        #modality_answer.append((row['what modality of data is used in this study?'].split(answer_start))[1].split(answer_end)[0])\n",
    "        #country_score.append((row['what country was the study conducted in?'].split(score_start))[1].split(score_end)[0])\n",
    "        #country_answer.append((row['what country was the study conducted in?'].split(answer_start))[1].split(answer_end)[0])    \n",
    "        #hospital_score.append((row['what hospital did the data come from?'].split(score_start))[1].split(score_end)[0])\n",
    "        #hospital_answer.append((row['what hospital did the data come from?'].split(answer_start))[1].split(answer_end)[0])\n",
    "        database_score.append((row[\"What existing data source did the data come from?\"].split(score_start))[1].split(score_end)[0])\n",
    "        database_answer.append((row[\"What existing data source did the data come from?\"].split(answer_start))[1].split(answer_end)[0])\n",
    "        organisation_score.append((row[\"What location did the data come from?\"].split(score_start))[1].split(score_end)[0])\n",
    "        organisation_answer.append((row[\"What location did the data come from?\"].split(answer_start))[1].split(answer_end)[0])  \n",
    "    except:\n",
    "        #disease_score.append('nan')\n",
    "        #disease_answer.append('nan')\n",
    "        sample_score.append('nan')\n",
    "        sample_answer.append('nan')\n",
    "        #question_score.append('nan')\n",
    "        #question_answer.append('nan')\n",
    "        #modality_score.append('nan')\n",
    "        #modality_answer.append('nan')\n",
    "        #country_score.append('nan')\n",
    "        #country_answer.append('nan') \n",
    "        #hospital_score.append('nan')\n",
    "        #hospital_answer.append('nan')\n",
    "        database_score.append('nan')\n",
    "        database_answer.append('nan')\n",
    "        organisation_score.append('nan')\n",
    "        organisation_answer.append('nan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bed5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "scoredf = cleandf[['pmid', 'title', 'methods']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3825d2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scoredf['disease_answer'] = disease_answer\n",
    "#scoredf['disease_answer'] = scoredf['disease_answer'].str[1:-1]\n",
    "#scoredf['disease_score'] = disease_score\n",
    "\n",
    "scoredf['sample_answer'] = sample_answer\n",
    "scoredf['sample_answer'] = scoredf['sample_answer'].str[1:-1]\n",
    "scoredf['sample_score'] = sample_score\n",
    "\n",
    "#scoredf['question_answer'] = question_answer\n",
    "#scoredf['question_answer'] = scoredf['question_answer'].str[1:-1]\n",
    "#scoredf['question_score'] = question_score\n",
    "\n",
    "#scoredf['modality_answer'] = modality_answer\n",
    "#scoredf['modality_answer'] = scoredf['modality_answer'].str[1:-1]\n",
    "#scoredf['modality_score'] = modality_score\n",
    "\n",
    "#scoredf['country_answer'] = country_answer\n",
    "#scoredf['country_answer'] = scoredf['country_answer'].str[1:-1]\n",
    "#scoredf['country_score'] = country_score\n",
    "\n",
    "#scoredf['hospital_answer'] = hospital_answer\n",
    "#scoredf['hospital_answer'] = scoredf['hospital_answer'].str[1:-1]\n",
    "#scoredf['hospital_score'] = hospital_score\n",
    "\n",
    "scoredf['database_answer'] = database_answer\n",
    "scoredf['database_answer'] = scoredf['database_answer'].str[1:-1]\n",
    "scoredf['database_score'] = database_score\n",
    "\n",
    "scoredf['organisation_answer'] = organisation_answer\n",
    "scoredf['organisation_answer'] = scoredf['organisation_answer'].str[1:-1]\n",
    "scoredf['organisation_score'] = organisation_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4b9341",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "scoredf.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5523b77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "scoredf.to_csv('output/methods_scored.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae49b4c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20f3129",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660132b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6f8aef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
